{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8t41QShcEiN"
      },
      "source": [
        "# General"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3NDlSV1avZq"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPbd5aNyavZr",
        "outputId": "7ee378fc-bf4c-40df-e25b-dcd666e7f1e4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import functools\n",
        "import numpy as np\n",
        "import dspy\n",
        "import phoenix as px\n",
        "\n",
        "from collections import Counter\n",
        "from typing import Dict, List\n",
        "from sklearn.metrics import homogeneity_score, completeness_score, v_measure_score\n",
        "from dspy.evaluate import Evaluate\n",
        "from dspy.functional import TypedPredictor\n",
        "from dspy.primitives.assertions import assert_transform_module, backtrack_handler\n",
        "from dspy.teleprompt import (\n",
        "    BootstrapFewShotWithRandomSearch,\n",
        "    BootstrapFewShot,\n",
        "    LabeledFewShot,\n",
        "    BootstrapFewShotWithOptuna,\n",
        "    COPRO,\n",
        "    BootstrapFinetune,\n",
        "    MIPRO,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUVK6YezavZr"
      },
      "source": [
        "## Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ROtQlATKavZs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "DATA_DIR = './data/'\n",
        "DATA_DIR = './data/derived_datasets/'\n",
        "PROP_CLUSTERING_PATH = os.path.join(DATA_DIR, 'proposition_clustering.json')\n",
        "SALIENCE_DETECTION_PATH =os.path.join(DATA_DIR, 'salience.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtMO1lR7avZt"
      },
      "source": [
        "## Phoenix & DSPy init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7lxkGgoavZt",
        "outputId": "3ebc7ccd-82d3-4e29-f4c7-d5f007ea8e80"
      },
      "outputs": [],
      "source": [
        "px.launch_app()\n",
        "from openinference.instrumentation.dspy import DSPyInstrumentor\n",
        "from opentelemetry import trace as trace_api\n",
        "from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n",
        "from opentelemetry.sdk import trace as trace_sdk\n",
        "from opentelemetry.sdk.resources import Resource\n",
        "from opentelemetry.sdk.trace.export import SimpleSpanProcessor\n",
        "\n",
        "endpoint = \"http://localhost:6006/v1/traces\"\n",
        "\n",
        "resource = Resource(attributes={})\n",
        "tracer_provider = trace_sdk.TracerProvider(resource=resource)\n",
        "span_otlp_exporter = OTLPSpanExporter(endpoint=endpoint)\n",
        "tracer_provider.add_span_processor(SimpleSpanProcessor(span_exporter=span_otlp_exporter))\n",
        "trace_api.set_tracer_provider(tracer_provider=tracer_provider)\n",
        "DSPyInstrumentor().instrument()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0H23yAKavZs"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7DulAGlavZs"
      },
      "source": [
        "### Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SuIbsPmCavZs"
      },
      "outputs": [],
      "source": [
        "prop_clustering = \"\"\n",
        "prop_clustering = \"\"\n",
        "\n",
        "with open(PROP_CLUSTERING_PATH, 'r') as f:\n",
        "    prop_clustering = json.load(f)\n",
        "\n",
        "with open(SALIENCE_DETECTION_PATH, 'r') as f:\n",
        "    salience = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTeQP7DwavZs",
        "outputId": "7bc5cde4-cf17-408d-a235-31f4a45a862d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Multinews (tests/ multi documents): 98\n",
            "Number of document: 275\n"
          ]
        }
      ],
      "source": [
        "def calculate_num_documents(dict_data):\n",
        "    return len([doc for test_text in dict_data.values() for doc in test_text[\"documents\"]])\n",
        "\n",
        "print(f'Number of Multinews (tests/ multi documents): {len(salience)}')\n",
        "print(f'Number of document: {calculate_num_documents(salience)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8HRF1wE_-Nx"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "bsCjrmM3_9nB"
      },
      "outputs": [],
      "source": [
        "# ChatGPT\n",
        "gpt3_turbo = dspy.OpenAI(model='gpt-3.5-turbo-instruct', max_tokens=1000)\n",
        "dspy.settings.configure(lm=gpt3_turbo, trace=[])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWILtpzyavZt"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpggsIMTavZt"
      },
      "source": [
        "### Salience"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxSpW2n5avZt"
      },
      "source": [
        "#### Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpWhVqMvavZu",
        "outputId": "e88ef85c-f850-4f2a-cfe2-e69911bc3144"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Maximum token limit: 8192\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoConfig\n",
        "\n",
        "# Load the tokenizer\n",
        "model_type = 'meta-llama/Meta-Llama-3-8B-Instruct'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_type)\n",
        "config = AutoConfig.from_pretrained(model_type)\n",
        "\n",
        "max_length = config.max_position_embeddings\n",
        "print(f\"Maximum token limit: {max_length}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwnSBrtYavZu",
        "outputId": "6779c87f-35d9-41b1-cf69-3d93bc515459",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test=test49, documents_tokens:[9755, 1747, 226, 127, 288], spans_tokens:992\n",
            "test=test34, documents_tokens:[7046, 1708, 46, 1662], spans_tokens:1037\n",
            "test=test59, documents_tokens:[1356, 113, 2768], spans_tokens:756\n",
            "test=test57, documents_tokens:[2744, 126, 45], spans_tokens:559\n",
            "test=test79, documents_tokens:[652, 4641, 845, 427], spans_tokens:672\n",
            "test=test95, documents_tokens:[228, 3221, 46, 58, 1049, 497], spans_tokens:702\n",
            "test=test50, documents_tokens:[3031, 384, 392], spans_tokens:402\n",
            "test=test63, documents_tokens:[3284, 12986], spans_tokens:578\n",
            "test=test78, documents_tokens:[5639, 484, 765], spans_tokens:1182\n",
            "test=test30, documents_tokens:[354, 389, 46, 432], spans_tokens:611\n",
            "test=test28, documents_tokens:[645, 229, 118, 311], spans_tokens:359\n",
            "test=test7, documents_tokens:[208, 248, 477], spans_tokens:550\n",
            "test=test11, documents_tokens:[267, 195], spans_tokens:122\n",
            "test=test46, documents_tokens:[200, 247], spans_tokens:258\n",
            "test=test12, documents_tokens:[220, 34, 213], spans_tokens:318\n",
            "test=test32, documents_tokens:[365, 176], spans_tokens:323\n",
            "test=test44, documents_tokens:[192, 119], spans_tokens:85\n",
            "test=test13, documents_tokens:[126, 125], spans_tokens:154\n",
            "test=test9, documents_tokens:[179, 1203, 614], spans_tokens:550\n",
            "test=test36, documents_tokens:[1000, 788], spans_tokens:253\n",
            "test=test4, documents_tokens:[1786, 1618, 109], spans_tokens:724\n",
            "test=test52, documents_tokens:[578, 407], spans_tokens:340\n",
            "test=test42, documents_tokens:[1196, 873], spans_tokens:404\n",
            "test=test45, documents_tokens:[2328, 679], spans_tokens:28\n",
            "test=test21, documents_tokens:[885, 726], spans_tokens:356\n",
            "test=test2, documents_tokens:[707, 1214], spans_tokens:328\n",
            "test=test41, documents_tokens:[1090, 647], spans_tokens:503\n",
            "test=test25, documents_tokens:[568, 222], spans_tokens:451\n",
            "test=test22, documents_tokens:[335, 415], spans_tokens:336\n",
            "test=test53, documents_tokens:[1306, 826], spans_tokens:309\n",
            "test=test40, documents_tokens:[67, 631, 100, 21], spans_tokens:61\n",
            "test=test26, documents_tokens:[1048, 672], spans_tokens:748\n",
            "test=test51, documents_tokens:[1189, 336], spans_tokens:721\n",
            "test=test0, documents_tokens:[1064, 1052], spans_tokens:337\n",
            "test=test37, documents_tokens:[640, 830, 502], spans_tokens:321\n",
            "test=test18, documents_tokens:[540, 347], spans_tokens:341\n",
            "test=test27, documents_tokens:[536, 730, 1555], spans_tokens:1290\n",
            "test=test39, documents_tokens:[251, 811], spans_tokens:193\n",
            "test=test17, documents_tokens:[376, 21], spans_tokens:433\n",
            "test=test29, documents_tokens:[919, 480], spans_tokens:448\n",
            "test=test31, documents_tokens:[422, 398, 445], spans_tokens:422\n",
            "test=test15, documents_tokens:[400, 337, 194], spans_tokens:395\n",
            "test=test23, documents_tokens:[657, 376, 1251], spans_tokens:444\n",
            "test=test24, documents_tokens:[403, 137, 567], spans_tokens:696\n",
            "test=test1, documents_tokens:[587, 212], spans_tokens:195\n",
            "test=test47, documents_tokens:[538, 1112], spans_tokens:448\n",
            "test=test14, documents_tokens:[504, 317], spans_tokens:375\n",
            "test=test38, documents_tokens:[518, 125], spans_tokens:198\n",
            "test=test20, documents_tokens:[2337, 407], spans_tokens:374\n",
            "test=test16, documents_tokens:[224, 381], spans_tokens:326\n",
            "test=test6, documents_tokens:[534, 637], spans_tokens:299\n",
            "test=test3, documents_tokens:[63, 1324, 741, 1501], spans_tokens:452\n",
            "test=test43, documents_tokens:[800, 849], spans_tokens:378\n",
            "test=test10, documents_tokens:[145, 296], spans_tokens:277\n",
            "test=test33, documents_tokens:[370, 123, 173, 45], spans_tokens:90\n",
            "test=test5, documents_tokens:[460, 927, 255], spans_tokens:595\n",
            "test=test19, documents_tokens:[561, 799], spans_tokens:208\n",
            "test=test48, documents_tokens:[624, 26], spans_tokens:204\n",
            "test=test55, documents_tokens:[148, 1059], spans_tokens:180\n",
            "test=test83, documents_tokens:[46, 132, 262], spans_tokens:219\n",
            "test=test58, documents_tokens:[815, 312], spans_tokens:593\n",
            "test=test67, documents_tokens:[173, 309, 536], spans_tokens:763\n",
            "test=test64, documents_tokens:[1240, 84, 692, 523, 1136, 853], spans_tokens:1189\n",
            "test=test72, documents_tokens:[811, 840], spans_tokens:686\n",
            "test=test61, documents_tokens:[46, 541, 267], spans_tokens:619\n",
            "test=test96, documents_tokens:[1860, 1246], spans_tokens:517\n",
            "test=test75, documents_tokens:[318, 316, 39, 632], spans_tokens:712\n",
            "test=test54, documents_tokens:[920, 679, 1283, 1116, 712], spans_tokens:1285\n",
            "test=test94, documents_tokens:[182, 469, 259, 407, 126, 864], spans_tokens:370\n",
            "test=test65, documents_tokens:[292, 395], spans_tokens:536\n",
            "test=test77, documents_tokens:[520, 208], spans_tokens:247\n",
            "test=test60, documents_tokens:[484, 485, 341], spans_tokens:934\n",
            "test=test56, documents_tokens:[613, 976, 46, 1850, 169, 45], spans_tokens:933\n",
            "test=test80, documents_tokens:[160, 1010, 46, 225], spans_tokens:521\n",
            "test=test90, documents_tokens:[345, 545], spans_tokens:547\n",
            "test=test91, documents_tokens:[702, 292, 218], spans_tokens:361\n",
            "test=test84, documents_tokens:[1309, 1746, 2039, 1733], spans_tokens:876\n",
            "test=test88, documents_tokens:[948, 1389], spans_tokens:424\n",
            "test=test93, documents_tokens:[527, 1141], spans_tokens:260\n",
            "test=test89, documents_tokens:[261, 176, 607, 736], spans_tokens:513\n",
            "test=test74, documents_tokens:[433, 1060], spans_tokens:609\n",
            "test=test98, documents_tokens:[117, 126, 126, 292, 412], spans_tokens:340\n",
            "test=test68, documents_tokens:[39, 882, 727], spans_tokens:645\n",
            "test=test71, documents_tokens:[317, 1899, 1004], spans_tokens:1041\n",
            "test=test87, documents_tokens:[153, 200, 309], spans_tokens:553\n",
            "test=test86, documents_tokens:[811, 779], spans_tokens:289\n",
            "test=test66, documents_tokens:[161, 483, 480], spans_tokens:738\n",
            "test=test92, documents_tokens:[628, 229], spans_tokens:513\n",
            "test=test81, documents_tokens:[150, 38], spans_tokens:108\n",
            "test=test97, documents_tokens:[9, 251, 790, 45], spans_tokens:461\n",
            "test=test82, documents_tokens:[5, 689, 844, 875], spans_tokens:1117\n",
            "test=test70, documents_tokens:[1056, 758, 63, 1102], spans_tokens:934\n",
            "test=test73, documents_tokens:[843, 38], spans_tokens:229\n",
            "test=test99, documents_tokens:[1583, 250], spans_tokens:309\n",
            "test=test62, documents_tokens:[326, 220], spans_tokens:438\n",
            "test=test69, documents_tokens:[872, 155], spans_tokens:461\n",
            "test=test85, documents_tokens:[17, 385, 957], spans_tokens:548\n",
            "test=test76, documents_tokens:[397, 226, 315], spans_tokens:742\n"
          ]
        }
      ],
      "source": [
        "# Counts the amount of tokens at each test\n",
        "for test, data in salience.items():\n",
        "    documents_tokens_amount = [len(tokenizer.tokenize(document)) for document in data[\"documents\"]]\n",
        "    spans_tokens_amount = len(tokenizer.tokenize(\" \".join([span[\"docSpanText\"] for span in data[\"salient_spans\"]])))\n",
        "\n",
        "    print(f\"test={test}, documents_tokens:{documents_tokens_amount}, spans_tokens:{spans_tokens_amount}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQ85ah5AavZu"
      },
      "source": [
        "#### Output Handeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pyvuh7IEavZu"
      },
      "source": [
        "##### Offsets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "iniPxkM8avZu"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def extract_saliences_from_output(output:str):\n",
        "    # Regular expression pattern to find saliences list\n",
        "    pattern = r'Saliences:.*'\n",
        "\n",
        "    # Find all matches\n",
        "    output = re.sub(r'\\s+', '', output)\n",
        "    output = output.replace(\"Saliences:\", \"\\nSaliences:\")\n",
        "    matches = re.findall(pattern, output)\n",
        "    spans = matches[-1]\n",
        "\n",
        "    pattern = r'\\(\\'(\\d+),(\\d+)-(\\d+)\\'\\)'\n",
        "    matches = re.findall(pattern, spans)\n",
        "\n",
        "    extracted_data = []\n",
        "    for match in matches:\n",
        "        id_value = int(match[0])\n",
        "        num1 = int(match[1])\n",
        "        num2 = int(match[2])\n",
        "        extracted_data.append((id_value, num1, num2))\n",
        "\n",
        "    extracted_spans = list(set(extracted_data))\n",
        "\n",
        "    return extracted_spans\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "fn_YU8o9avZu"
      },
      "outputs": [],
      "source": [
        "def load_spans_from_documents(documents, spans):\n",
        "    output_spans = []\n",
        "    for span in spans:\n",
        "        doc_id, start_range, end_range = span\n",
        "\n",
        "        if doc_id < len(documents):\n",
        "            output_spans.append(documents[doc_id][start_range:end_range])\n",
        "\n",
        "    return output_spans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "fl5SVTpsavZu"
      },
      "outputs": [],
      "source": [
        "def convert_output_span_indexes_to_spans(documents, output_span_indexes):\n",
        "    spans = extract_saliences_from_output(output_span_indexes)\n",
        "    return load_spans_from_documents(documents, spans)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeHjyXjkavZu"
      },
      "source": [
        "##### Full texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "AU5f6ntvavZv"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "# General model\n",
        "def extract_target_saliences_from_output(output):\n",
        "    keyword = \"Saliences:\"\n",
        "    last_index = output.rfind(keyword)\n",
        "\n",
        "    if last_index != -1:\n",
        "        substring = output[last_index + len(keyword):].strip()\n",
        "        substring = substring.strip('\"\\n')\n",
        "\n",
        "        if \"[\" in substring and \"]\" in substring:\n",
        "            pattern = r'\\[(.*?)\\]'\n",
        "            matches = re.findall(pattern, substring)\n",
        "            substring = matches[0]\n",
        "\n",
        "        return [span.strip('\"\\n') for span in substring.split(\"\\\", \\\"\")]\n",
        "\n",
        "    return None  # Handle case where keyword is not found\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VX-7G8ZlavZv"
      },
      "source": [
        "#### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "PC_dataset = []\n",
        "\n",
        "def clean_str(string :str):\n",
        "    return string#.replace(r'.', r',').replace(r',,', r',').replace(r'\"', r' ')\n",
        "\n",
        "for topic in prop_clustering.keys():\n",
        "    current = prop_clustering[topic]\n",
        "    docSpanTexts = [item['docSpanText'] for item in  current['input_spans']]\n",
        "    random.shuffle(docSpanTexts)\n",
        "    spans_list = {item : index for index, item in zip(range(len(docSpanTexts)), docSpanTexts)}\n",
        "    # input_spans = [clean_str(item) for item in docSpanTexts]\n",
        "    input_spans = [f'<START_SPAN_{spans_list[item]}>: {clean_str(item)} <END_SPAN_{spans_list[item]}>' for item in docSpanTexts]\n",
        "    clusters = []\n",
        "    for cluster in prop_clustering[topic]['clusters']:\n",
        "        salience_spans = {spans_list[span['docSpanText']]:int(cluster[\"clusterID\"]) for span in cluster['spans']}\n",
        "        clusters.append(salience_spans)\n",
        "    flatten_cluster = str({k: v for cluster in clusters for k,v in cluster.items()})\n",
        "    PC_dataset.append(dspy.Example(salience_spans=' '.join(input_spans), clusters=flatten_cluster).with_inputs('salience_spans'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "40i9K6COavZv",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Removes large documents from the dataset\n",
        "threshold = 1000\n",
        "\n",
        "new_salience_data = {}\n",
        "salience_dataset = []\n",
        "old_to_new_doc_index = {}\n",
        "for test, data in salience.items():\n",
        "    # spans = data[\"\"]\n",
        "    spans = data[\"salient_spans\"]\n",
        "    new_documents = []\n",
        "    for index, document in enumerate(data[\"documents\"]):\n",
        "        if len(tokenizer.tokenize(document)) > threshold:\n",
        "            new_spans = []\n",
        "            for span in spans:\n",
        "                if span[\"documentFile\"].split(\".\")[0] != str(index):\n",
        "                    new_spans.append(span)\n",
        "            spans = new_spans\n",
        "        else:\n",
        "            curr_index = len(new_documents)\n",
        "            old_to_new_doc_index[index] = curr_index\n",
        "            new_documents.append(document)\n",
        "\n",
        "    if len(new_documents) >= 2:\n",
        "        new_spans = []\n",
        "        for span in spans:\n",
        "            record = {}\n",
        "            record[\"documentFile\"] = old_to_new_doc_index[int(span['documentFile'].split('.')[0])]\n",
        "            record[\"docSpanOffsets\"] = span['docSpanOffsets'].replace(', ', '-')\n",
        "            record[\"docSpanText\"] = span['docSpanText']\n",
        "            new_spans.append(record)\n",
        "\n",
        "        salience_dataset.append(dspy.Example(documents=new_documents, saliences=new_spans).with_inputs('documents'))\n",
        "        new_salience_data[test] = {\n",
        "            \"documents\": new_documents,\n",
        "            \"salient_spans\": spans,\n",
        "        }\n",
        "\n",
        "new_salience_dataset = []\n",
        "for example in salience_dataset:\n",
        "    if len(example.documents) <=1 or len(example.saliences) <= 2:\n",
        "        continue\n",
        "    new_salience_dataset.append(example)\n",
        "\n",
        "salience_dataset = new_salience_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "q7VYTo8KavZv"
      },
      "outputs": [],
      "source": [
        "num_topics = len([topic for topic in new_salience_data.keys()])\n",
        "train_size, dev_size = int(num_topics * 0.20), int(num_topics * 0.35)\n",
        "test_size = num_topics - train_size - dev_size\n",
        "\n",
        "def train_test_split(data_dict, start_index, end_index):\n",
        "    items = list(data_dict.items())\n",
        "    split_data = dict(items[start_index: end_index])\n",
        "\n",
        "    return split_data\n",
        "\n",
        "salience_train = salience_dataset[:train_size]\n",
        "salience_dev   = salience_dataset[train_size: train_size + dev_size]\n",
        "salience_test  = salience_dataset[train_size + dev_size:]\n",
        "\n",
        "train_PC = PC_dataset[:train_size]\n",
        "val_PC   = PC_dataset[train_size: train_size + dev_size]\n",
        "test_PC  = PC_dataset[train_size + dev_size:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IATz0LpOavZv",
        "outputId": "cadfbfb9-aecd-4417-baa4-b2aa8d4d8a60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train size: 15\n",
            "Dev size: 26\n",
            "Test size: 33\n"
          ]
        }
      ],
      "source": [
        "print(f'Train size: {len(salience_train)}')\n",
        "print(f'Dev size: {len(salience_dev)}')\n",
        "print(f'Test size: {len(salience_test)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnfsjTMfavZv"
      },
      "source": [
        "#### Metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "_mU53DB-avZv"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "\n",
        "def convert_output_to_list(output):\n",
        "\n",
        "    output = re.sub(r'\\[(\\w+)\\]', r'<\\1>', output)\n",
        "    matches =  re.findall(r'\\[(.*?)\\]', output, re.DOTALL)\n",
        "\n",
        "    if matches:\n",
        "        output = matches[0]\n",
        "    else:\n",
        "      if output[-1] == '\\'' or  output[-1] == '\\\"' or output[-1] == '\\n':\n",
        "        output += \"]\"\n",
        "      else:\n",
        "        output += \"']\"\n",
        "\n",
        "    return eval(output)\n",
        "\n",
        "def metric(real_spans, pred_spans, trace=None):\n",
        "    # Tokenize the real and predicted spans\n",
        "    real_tokens = [token for span in real_spans.saliences for token in tokenizer.tokenize(span[\"docSpanText\"])]\n",
        "    pred_tokens = [token for span in convert_output_to_list(pred_spans.saliences) for token in tokenizer.tokenize(span)]\n",
        "\n",
        "\n",
        "    # Not optimal\n",
        "    min_length = min(len(real_tokens), len(pred_tokens))\n",
        "    real_tokens = real_tokens[:min_length]\n",
        "    pred_tokens = pred_tokens[:min_length]\n",
        "\n",
        "    # Count the occurrences of each token in real and predicted tokens\n",
        "    real_counter = Counter(real_tokens)\n",
        "    pred_counter = Counter(pred_tokens)\n",
        "\n",
        "    # Calculate True Positives (TP), False Positives (FP), and False Negatives (FN)\n",
        "    TP = sum((real_counter & pred_counter).values())\n",
        "    FP =  sum([(pred_counter - real_counter)[key] for key in pred_counter if key not in (pred_counter.keys() - real_counter.keys())])\n",
        "    FN =  sum([pred_counter[key] for key in pred_counter.keys() - real_counter.keys()])\n",
        "\n",
        "    # Calculate Precision, Recall, and F1 Score\n",
        "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    return  f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oMsfAkHavZv"
      },
      "source": [
        "#### Signature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pydantic\n",
        "\n",
        "class DictOfSpansClustersType(pydantic.BaseModel):\n",
        "    span_cluster_dict: Dict[int, int]\n",
        "\n",
        "class PropositionClusteringSignature(dspy.Signature):\n",
        "    \"\"\"Please Cluster salience spans into groups. Each group should contain spans that share the same information. all the input spans should be in the output clusters\"\"\"\n",
        "\n",
        "    # salience_spans = dspy.InputField()\n",
        "    # salience_spans = dspy.InputField(format=list)\n",
        "    salience_spans : str = dspy.InputField(desc='Salience spans which each span starting with <START_SPAN_IDX> and end with <END_SPAN_IDX>, each salience span can have <SPAN_SEP> which seprate bettwen subspans inside the currnet IDX span.')\n",
        "    # output_length : str = dspy.InputField(desc='Fact of the expected output length')\n",
        "    clusters : Dict[int, int] = dspy.OutputField(desc='A dict in the following format {<SPAN_NUMBER_IDXi>:<CLUSTER IDXi>, <SPAN_NUMBER_IDXj>:<CLUSTER IDXj>, ...}. return ONLY a dict with spans and cluster!, return ONLY a dict with spans and cluster!, return ONLY a dict with spans and cluster!, return ONLY a dict with spans and cluster!')\n",
        "    # clusters : Dict[int, int] = dspy.OutputField()\n",
        "    # clusters : DictOfSpansClustersType = dspy.OutputField()\n",
        "\n",
        "class CleanDictSignature(dspy.Signature):\n",
        "    \"\"\"Clean the dict to be only integers. The keys and the values must be type int, Return ONLY dict.\n",
        "\n",
        "    ---\n",
        "\n",
        "    Follow the examples:\n",
        "    String dict: {\"0\": \"cluster1\",\"1\": \"cluster2\",\"2\": \"cluster3\",\"3\": \"cluster3\",\"4\": \"cluster3\",\"5\": \"cluster3\",\"6\": \"cluster3\",\"7\": \"cluster3\",\"8\": \"cluster3\",\"9\": \"cluster3\",\"10\": \"cluster3\",\"11\": \"cluster3\",\"12\": \"cluster3\",\"13\": \"cluster3\",\"14\": \"cluster4\"}\n",
        "    Int dict:{0: 1,1: 2,2: 3,3: 3,4: 3,5: 3,6: 3,7: 3,8: 3,9: 3,10: 3,11: 3,12: 3,13: 3,14: 4}\n",
        "    \n",
        "\n",
        "    String dict: { '0': 0, '1': 0, '2': 0, '3': 0, '4': 0, '5': 0, '6': 0, '7': 0, '8': 0, '9': 0, '10': 0, '11': 0, '12': 1, '13': 1, '14': 2 }\n",
        "    Int dict::{ 0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 1, 13: 1, 14: 2 }\n",
        "    \n",
        "\n",
        "    String dict: \"{ 'SPAN_NUMBER_0': 'CLUSTER_1', 'SPAN_NUMBER_1': 'CLUSTER_2', 'SPAN_NUMBER_2': 'CLUSTER_2', 'SPAN_NUMBER_3': 'CLUSTER_2', 'SPAN_NUMBER_4': 'CLUSTER_2', 'SPAN_NUMBER_5': 'CLUSTER_2', 'SPAN_NUMBER_6': 'CLUSTER_2', 'SPAN_NUMBER_7': 'CLUSTER_2', 'SPAN_NUMBER_8': 'CLUSTER_3', 'SPAN_NUMBER_9': 'CLUSTER_4', 'SPAN_NUMBER_10': 'CLUSTER_4' }\"\n",
        "    Int dict:  { 0: 1, 1: 2, 2: 2, 3: 2, 4: 2, 5: 2, 6: 2, 7: 2, 8: 3, 9: 4, 10: 4 }\n",
        "\n",
        "    \n",
        "    String dict: \"{<START_SPAN_0>: 0, <START_SPAN_1>: 0, <START_SPAN_2>: 1, <START_SPAN_3>: 0, <START_SPAN_4>: 2, <START_SPAN_5>: 2, <START_SPAN_6>: 2, <START_SPAN_7>: 1, <START_SPAN_8>: 2, <START_SPAN_9>: 2, <START_SPAN_10>: 3, <START_SPAN_11>: 2}\"}\n",
        "    Int dict:  {0: 0, 1: 0, 2: 1, 3: 0, 4: 2, 5: 2, 6: 2, 7: 1, 8: 2, 9: 2, 10: 3, 11: 2}\n",
        "    ---\n",
        "    \"\"\"\n",
        "    \n",
        "\n",
        "    string_dict = dspy.InputField(desc='str dict which need to be cleaned up')\n",
        "    int_dict = dspy.OutputField(desc='A dict with keys as type int, and values as type int.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PropositionalClusteringCOT(dspy.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.generate_clusters = dspy.ChainOfThoughtWithHint(PropositionClusteringSignature, activated=False)\n",
        "        self.clean_clusters = dspy.Predict(CleanDictSignature)\n",
        "\n",
        "    def forward(self, salience_spans):\n",
        "        clusters = self.generate_clusters(salience_spans=salience_spans, hint=f'Output length should be {len(salience_spans)} spans.')\n",
        "        try:\n",
        "            eval(clusters.clusters)\n",
        "            return dspy.Prediction(clusters=clusters.clusters)\n",
        "        except:\n",
        "            pass\n",
        "        # with dspy.\n",
        "        int_clusters = self.clean_clusters(string_dict=clusters.clusters)\n",
        "        pred = dspy.Prediction(clusters=int_clusters.int_dict)\n",
        "        return pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import homogeneity_score, completeness_score, v_measure_score\n",
        "import numpy as np\n",
        "from dspy.evaluate import Evaluate\n",
        "\n",
        "def validate_propositional_clustering(example, pred, trace=None):\n",
        "    \"\"\"Validate clustering results using homogeneity, completeness, and V-measure.\"\"\"\n",
        "    \n",
        "    true_labels = eval(example.clusters)\n",
        "    predicted_labels = eval(pred.clusters)\n",
        "    # print(pred.clusters)\n",
        "    # list of cluster by span number if key=span_idx : value=cluster_idx if {1:0, 2:0, 0:1} then [1, 0, 0]\n",
        "    true_labels_list = [true_labels[key] for key in sorted(true_labels.keys())]\n",
        "    predicted_labels_list = [predicted_labels[key] for key in sorted(predicted_labels.keys())]\n",
        "    # print(true_labels_list)\n",
        "    # print(predicted_labels_list)\n",
        "    if np.shape(true_labels_list) != np.shape(predicted_labels_list):\n",
        "        print(f'Shape inst match.')\n",
        "        # print(f'true_labels_list: {true_labels_list}, np.shape(true_labels_list)={np.shape(true_labels_list)}, \\npredicted_labels_list:{predicted_labels_list}, np.shape(predicted_labels_list)={np.shape(predicted_labels_list)}')\n",
        "        return 0\n",
        "    homogeneity = homogeneity_score(true_labels_list, predicted_labels_list)\n",
        "    completeness = completeness_score(true_labels_list, predicted_labels_list)\n",
        "    v_measure = v_measure_score(true_labels_list, predicted_labels_list)\n",
        "    \n",
        "    print({\n",
        "        'homogeneity': homogeneity,\n",
        "        'completeness': completeness,\n",
        "        'v_measure': v_measure\n",
        "    })\n",
        "    # return homogeneity + completeness + v_measure\n",
        "    return {\n",
        "        'homogeneity': homogeneity,\n",
        "        'completeness': completeness,\n",
        "        'v_measure': v_measure\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "nM-Vr3_javZw"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "\n",
        "def format_output(data):\n",
        "    if isinstance(data, list):\n",
        "        return [record['docSpanText'] for record in data]\n",
        "    return data\n",
        "\n",
        "class SalienceSignature(dspy.Signature):\n",
        "    \"\"\"Below are documents on the same topic in different user messages. Please copy exactly salient sub-sentenial spans. Do not change the copied text.\"\"\"\n",
        "    documents = dspy.InputField(desc=\"Documents containing News information\",\n",
        "                                format=lambda x: x if isinstance(x, str) else list(x))\n",
        "    saliences = dspy.OutputField(\n",
        "        desc=\"Extracted sentences formated as python list, sentences copied as-is without any modification!\",\n",
        "        format=format_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "def validate_correct_list(output):\n",
        "  try:\n",
        "      convert_output_to_list(output)\n",
        "      return True\n",
        "  except:\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "kwXKcVhMVLSM"
      },
      "outputs": [],
      "source": [
        "class SaliencePredict(dspy.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # self.pred = dspy.TypedPredictor(SalienceSignature)\n",
        "        # self.pred =  dspy.Predict(SalienceSignature, temperature=0.05)\n",
        "        self.pred =  dspy.Predict(SalienceSignature,  temperature=0.5 , max_tokens=1000)\n",
        "\n",
        "    def forward(self, documents):\n",
        "      output = self.pred(documents=documents)\n",
        "      dspy.Suggest(validate_correct_list(output.saliences), \"Make sure Saliences is a valid json list of strings\")\n",
        "      return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "bHocPbjtVn5X"
      },
      "outputs": [],
      "source": [
        "def convert_output_to_list(output):\n",
        "    print(1)\n",
        "    output = re.sub(r'\\[(\\w+)\\]', r'<\\1>', output)\n",
        "    print(2)\n",
        "    print(output)\n",
        "    matches =  re.findall(r'\\[(.*?)\\]', output, re.DOTALL)\n",
        "    print(3)\n",
        "\n",
        "    if matches:\n",
        "      print(4)\n",
        "      output = matches[0]\n",
        "      print(len(matches))\n",
        "    else:\n",
        "      print(5)\n",
        "      if output[-1] == '\\'' or  output[-1] == '\\\"' or output[-1] == '\\n':\n",
        "        print(7)\n",
        "        output += \"]\"\n",
        "      else:\n",
        "        print(8)\n",
        "        output += \"']\"\n",
        "\n",
        "    print(output)\n",
        "    return eval(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFQCD8ZyAirH"
      },
      "source": [
        "#### Assertion & Predictor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Pipeline(dspy.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.salience_detection = assert_transform_module(SaliencePredict(), functools.partial(backtrack_handler, max_backtracks=1))\n",
        "        self.salience_detection.load('./salience_optimized.json')\n",
        "        self.propositional_clustering = PropositionalClusteringCOT()\n",
        "        self.propositional_clustering.load('./propositional_clustering.json')\n",
        "\n",
        "    def forward(self, documents):\n",
        "      saliences = self.salience_detection(documents=documents)\n",
        "      clusters = self.propositional_clustering(salience_spans=convert_output_to_list(saliences.saliences))\n",
        "      return clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "with_assertions = assert_transform_module(SaliencePredict(), functools.partial(backtrack_handler, max_backtracks=1))\n",
        "with_assertions.load('./salience_optimized.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "pred = Retry(StringSignature(documents -> saliences\n",
              "    instructions='Below are documents on the same topic in different user messages. Please copy exactly salient sub-sentenial spans. Do not change the copied text.'\n",
              "    documents = Field(annotation=str required=True json_schema_extra={'desc': 'Documents containing News information', 'format': <function SalienceSignature.<lambda> at 0x000001B6B1653790>, '__dspy_field_type': 'input', 'prefix': 'Documents:'})\n",
              "    saliences = Field(annotation=SalienceOutput required=True json_schema_extra={'desc': 'Extracted sentences formated as python list (note strings must be inside double quotation marks), sentences copied as-is without any modification!', 'format': <function format_output at 0x000001B6B16539D0>, '__dspy_field_type': 'output', 'prefix': 'Saliences:'})\n",
              "))"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with_assertions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = PropositionalClusteringCOT()\n",
        "model.load('./propositional_clustering.json')\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline = Pipeline()\n",
        "pipeline(**salience_test[0].inputs())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "prop_clustering = PropositionalClusteringCOT().load('./propositional_clustering.json')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "salience_scores = []\n",
        "with_assertions = assert_transform_module(SaliencePredict(), functools.partial(backtrack_handler, max_backtracks=1))\n",
        "with_assertions.load('./salience_optimized.json')\n",
        "for example in salience_test:\n",
        "    pred = with_assertions(**example.inputs())\n",
        "    score = metric(example, pred)\n",
        "    salience_scores.append(score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'homogeneity': 0.9999999999999998, 'completeness': 0.8343604297372121, 'v_measure': 0.9097017316893838}\n",
            "{'homogeneity': 1.0, 'completeness': 0.8524398460347836, 'v_measure': 0.9203428093597348}\n",
            "{'homogeneity': 1.0, 'completeness': 0.8077698004717359, 'v_measure': 0.8936644480519025}\n",
            "{'homogeneity': 0.5734773267433709, 'completeness': 0.722346144208289, 'v_measure': 0.6393604453849324}\n",
            "{'homogeneity': 1.0, 'completeness': 0.9687500000000001, 'v_measure': 0.9841269841269843}\n",
            "Shape inst match.\n",
            "{'homogeneity': 0.8333333333333333, 'completeness': 1.0, 'v_measure': 0.9090909090909091}\n",
            "{'homogeneity': 1.0, 'completeness': 0.9624786378518295, 'v_measure': 0.9808806264565294}\n",
            "{'homogeneity': 1.0, 'completeness': 0.8755776673507596, 'v_measure': 0.933661860654917}\n",
            "{'homogeneity': 1.0, 'completeness': 0.8595824360039366, 'v_measure': 0.9244897342126939}\n",
            "{'homogeneity': 1.0000000000000002, 'completeness': 0.9424351665604174, 'v_measure': 0.9703646049914161}\n",
            "{'homogeneity': 0.9999999999999999, 'completeness': 0.8659123211672697, 'v_measure': 0.9281382746061464}\n",
            "{'homogeneity': 1.0000000000000002, 'completeness': 0.7748718271109772, 'v_measure': 0.873158067275499}\n",
            "{'homogeneity': 1.0, 'completeness': 1.0, 'v_measure': 1.0}\n",
            "{'homogeneity': 1.0000000000000002, 'completeness': 0.7687798533178652, 'v_measure': 0.8692770350994142}\n",
            "{'homogeneity': 1.0, 'completeness': 0.9397940008672038, 'v_measure': 0.9689626841273452}\n",
            "{'homogeneity': 1.0, 'completeness': 1.0, 'v_measure': 1.0}\n",
            "{'homogeneity': 1.0, 'completeness': 0.7591760034688151, 'v_measure': 0.8631040918837465}\n",
            "{'homogeneity': 0.9487436426212903, 'completeness': 0.9487436426212901, 'v_measure': 0.9487436426212902}\n",
            "{'homogeneity': 1.0, 'completeness': 0.742565976271254, 'v_measure': 0.8522672729559406}\n",
            "{'homogeneity': 1.0, 'completeness': 0.7755566466883426, 'v_measure': 0.8735926821989739}\n",
            "Shape inst match.\n",
            "{'homogeneity': 1.0, 'completeness': 0.7025814841677875, 'v_measure': 0.8253131972843056}\n",
            "{'homogeneity': 1.0000000000000002, 'completeness': 0.750785606369205, 'v_measure': 0.8576556759867259}\n",
            "{'homogeneity': 0.9999999999999999, 'completeness': 0.890950412962958, 'v_measure': 0.9423308055623888}\n",
            "{'homogeneity': 0.9999999999999999, 'completeness': 0.852678349986448, 'v_measure': 0.9204817986810124}\n",
            "Shape inst match.\n",
            "{'homogeneity': 1.0, 'completeness': 0.8134791926692312, 'v_measure': 0.8971475338207593}\n",
            "Shape inst match.\n",
            "{'homogeneity': 1.0, 'completeness': 0.8224908743082041, 'v_measure': 0.9026008150744917}\n",
            "{'homogeneity': 0.9999999999999999, 'completeness': 0.7456873716609572, 'v_measure': 0.8543194890061708}\n",
            "{'homogeneity': 1.0, 'completeness': 0.7545468576838044, 'v_measure': 0.860104538535824}\n",
            "{'homogeneity': 0.9999999999999998, 'completeness': 0.8164417563493365, 'v_measure': 0.8989462541207063}\n",
            "{'homogeneity': 1.0, 'completeness': 0.7807529906804597, 'v_measure': 0.8768796062862364}\n",
            "{'homogeneity': 1.0000000000000002, 'completeness': 0.798230985584246, 'v_measure': 0.8877958304393254}\n",
            "{'homogeneity': 0.9776929721204488, 'completeness': 0.6036762219648258, 'v_measure': 0.7464544040174436}\n",
            "{'homogeneity': 1.0000000000000002, 'completeness': 0.8123931892591477, 'v_measure': 0.8964866940282753}\n",
            "{'homogeneity': 0.9999999999999998, 'completeness': 0.7591760034688151, 'v_measure': 0.8631040918837464}\n",
            "{'homogeneity': 0.9999999999999999, 'completeness': 0.8659123211672697, 'v_measure': 0.9281382746061464}\n",
            "{'homogeneity': 1.0000000000000002, 'completeness': 0.7695459427785482, 'v_measure': 0.869766558951506}\n",
            "{'homogeneity': 0.9999999999999998, 'completeness': 0.8293613167934563, 'v_measure': 0.9067222633166623}\n",
            "{'homogeneity': 1.0000000000000002, 'completeness': 0.8051515764715411, 'v_measure': 0.8920597992611118}\n",
            "{'homogeneity': 1.0, 'completeness': 0.7931563623800861, 'v_measure': 0.8846482984086413}\n",
            "Shape inst match.\n",
            "{'homogeneity': 0.9298966940476159, 'completeness': 1.0, 'v_measure': 0.9636750992068105}\n",
            "{'homogeneity': 0.9999999999999997, 'completeness': 0.8163048441057034, 'v_measure': 0.8988632571837119}\n",
            "{'homogeneity': 1.0, 'completeness': 0.7693629238533843, 'v_measure': 0.869649650143948}\n",
            "{'homogeneity': 0.5051961085524235, 'completeness': 0.7082316448032829, 'v_measure': 0.5897275217561565}\n",
            "Shape inst match.\n",
            "{'homogeneity': 1.0, 'completeness': 0.7492108319082036, 'v_measure': 0.8566272495476077}\n",
            "{'homogeneity': 0.9999999999999999, 'completeness': 0.7438195036979354, 'v_measure': 0.853092309290721}\n",
            "{'homogeneity': 1.0000000000000002, 'completeness': 0.7937397841287682, 'v_measure': 0.8850110714518084}\n",
            "{'homogeneity': 0.7689510034106667, 'completeness': 0.8827345872551081, 'v_measure': 0.8219235554891632}\n",
            "{'homogeneity': 0.9999999999999999, 'completeness': 0.6753934693979577, 'v_measure': 0.8062505694744724}\n",
            "{'homogeneity': 0.9999999999999994, 'completeness': 0.7884474843322408, 'v_measure': 0.8817116423484206}\n",
            "{'homogeneity': 0.9999999999999997, 'completeness': 0.913268476686875, 'v_measure': 0.9546683989361939}\n",
            "{'homogeneity': 1.0, 'completeness': 0.8309518296808298, 'v_measure': 0.9076719728073684}\n"
          ]
        }
      ],
      "source": [
        "scores = []\n",
        "prop_clustering = PropositionalClusteringCOT()\n",
        "prop_clustering.load('./propositional_clustering.json')\n",
        "for example in test_PC:\n",
        "    pred = prop_clustering(**example.inputs())\n",
        "    score = validate_propositional_clustering(example, pred)\n",
        "    scores.append(score)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "F8t41QShcEiN",
        "TxSpW2n5avZt",
        "bQ85ah5AavZu",
        "VX-7G8ZlavZv",
        "gnfsjTMfavZv"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
